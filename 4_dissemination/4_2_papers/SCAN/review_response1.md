First of all, we would like to thank the editor and reviewers for their detailed comments, which we have found extremely useful in revising and improving the quality of the paper. 

For convenience we have included original comments indented, with responses following.

 

## Editor’s Comments

**E.1** 

>…both reviewers noted some confusion about the timeline of the study and, in particular, when assessments happened with respect to each other. This is probably the most substantial critique because it bears both on how you interpret the results (see R2 for more thoughts on that) and on what you mean by "transient" and "lasting". I think a figure would go a long way here, as well as more careful attention to the terminology of the "liking" versus "tastiness" ratings, and increased clarity about what happened outside of the scanner before the scan, inside the scan, and outside the scanner after the scan. (Both reviewers also point out some confusion between "liking" and "wanting in various places.)

We have added a figure (see below) that outlines the study timeline which we hope helps clarify when assessments occurred. In terms of how this timing affects our interpretation of results we attempt to address this in our response to reviewer 2. 

We agree that we should make a clearer distinction between "liking" and "tastiness" and "liking" and "wanting".

**E.2** 

> The other main issue is the way the analyses were set up with respect to the direction of the predictions. 

**E.2a**

> ...it wasn't always clear which ratings were being entered in the models. Clarity in the terminology would help the reader map different assessments to the various terms in the model. 

 



**E.2b**

> ...the directionality. As you know, the DV of a regression model is the outcome and the IV(s) is/are the predictors. So, intuitively, if the question is whether brain activation predicts (changes in) liking/wanting, then those ratings ought to be the DV with the neural betas as the IVs. The fact that this was reversed in a few analyses will imply to some readers that the neural activation was being predicted by the ratings, which took place after the acquisition of the neural data in some cases. See Reviewer 2's comments on this point for additional thoughts.

 

 

## Reviewers’ Comments

### Reviewer 1

#### Summary

> The objective of the present manuscript is to investigate how brain activity associated with the regulation of dietary decision making affects longer-term food preferences. Overall, I found this to be a well-written manuscript describing a well-designed study, and one that I think advances the literature in this area.

 

#### General Notes

> I suggest revising the text to improve clarity of some important constructs, especially the present operationalization of “durability” and “transience” of self-regulatory effects. There are also some additional areas that I think would benefit from clarification.

#### Introduction

**R1.1**

> In the end of the introduction (p. 7), the authors state that their goal was “to determine whether regulation alters the independent influences of vmPFC and dlPFC on choice behavior,” and in the next sentence state that “this route may act as a compensatory mechanism that recruits the dlPFC…” Which route? And how does a hypothesized compensatory mechanism relate to the aforementioned investigation into independent influences of the vmPFC and dlPFC? This section, which is critical to the presentation and framing of the present study, is unclear.



#### Methods

**R1.2** 

> More detail is needed regarding the participants. For example, what was their race/ethnicity? How were they recruited?

Additional demographic information about participants has been added to the methods section. They were recruited via both postering and Facebook groups. 



**R1.3**

> Was a power analysis conducted to determine the sample size a priori? If not, how was the N determined?

We now mention in the methods that sample size was determined a priori by matching the sample size that had successfully found effects in previous studies using a very similar task structure. We were not able to go above this number as it was the lagest N consistent with our budget.



**R1.4**

> The overview section states that the subjects “rated a set of foods for momentary liking.” Further reading of the methods reveals that these are the same 270 stimuli that were used in the regulation task, however that information would be beneficial to the reader’s understanding of the overall study design if presented in the overview as well.

This has been addressed.

 

**R1.5**

> In the Cognitive Regulation Task, was each food type and/or stimulus only seen under one instruction?

 Yes, this has been clarified in methods.



**R1.6**

> While the overview states that the Liking Rating Task is administered before and after the scanning session, it may be easier for the reader if the fact that this is a non-scanner task is repeated in the description of the task.

 Agreed, we have clarified this point.



**R1.7**

> Were the stimuli shown in the Liking Rating Task identical to the 270 used in the Cognitive Regulation Task? I assume so, but it would be helpful to explicitly say.

Yes, and we have clarified this in the methods.

 

**R1.8**

> In the Attribute Ratings Task description, the authors say that “participants viewed all foods again” – was this the 135 foods or the 270 stimuli (all foods seen in two different amounts)?

It was the 270 stimuli (135 foods in two different quantities). We have clarified this point in the methods.

 

**R1.9**

> How do the authors operationalize “transience” or “durability” of liking (or alternately, self-regulation)? Since this is a main outcome of the current study, a clear explanation in the methods section of how these effects are defined is needed.

We have attempted first of all to clean up our language and now refer only to momentary and lasting regulatory changes. We provide explicit descriptions of how we are using these words and avoid synonyms that could create confusion. We define momentary regulatory changes as the conditional effects on choices subjects make while in-scanner in both of our regulatory conditions (DECREASE and HEALTH). We define lasting regulatory changes as a consistent change in liking ratings of food items after the completion of the in-scanner task, that is also correlated with the condition in which the food item was viewed. We also specify the amount of time that elapsed between the in-scanner task and the re-ratings of the food stimuli for liking in order to give "lasting" a determinate time period. 

 

**R1.10**

> Was FMRIPREP run using the default (or most common) settings? While the FMRIPREP details are useful, it would help the reader to interpret the methods if deviations from the defaults are noted.

FMRIPREP was run using default settings, this has been noted.

 

**R1.11**

> How many runs were removed due to >30% of excluded frames? How many subjects were removed due to having more than 3/9 runs removed? I assume this is the same number listed in the participants section, but it would be helpful to repeat this information on p. 15.

54 runs were removed to to the head movement exclusion criterion. This information has been added with additional detail on number of trials exluded by subject. We repeated the information about removing 11 subjects due to excessive head motion here.

 

**R1.12**

> How were parameter contrast estimates extracted?

 Cendri indicated she would add.



#### Results

**R1.13**

> It might be useful for future meta-analyses, etc. for the authors to include tables summarizing the results of the whole-brain GLMs.



All Preference:

| Brain Region     | Side | BA          | k     | equivZ | MNI  |      |      |
| ---------------- | ---- | ----------- | ----- | ------ | ---- | ---- | ---- |
|                  |      |             |       |        | x    | y    | z    |
| Ventromedial PFC | R/L  | 24/32/10/11 | 11454 | 5.84   | -4   | 36   | 12   |
|                  |      |             |       | 5.47   | -8   | 50   | -2   |
|                  |      |             |       | 5.27   | -2   | -4   | 32   |
| Dorsolateral PFC | R    | 46          | 636   | 4.45   | 42   | 36   | 18   |
|                  | L    | 9           | 658   | 4.06   | -54  | 24   | 36   |



#### Discussion

**R1.14**

> The authors do a lovely job of synthesizing the present findings with the current literature and motivating future directions. However, the discussion lacks mention of any limitations of the study, and would benefit from balancing the implications of these findings with any shortcomings of the present work.

We add what we believe to be meaningful limitations to our study in the discussion. Specifically we point out:

- Weaknesses of state induction methods
- Limited timeframe tested for the lasting effects of regulation
- Did not test for attribute ratings (health and taste) pre-scan
- Analysis only focussed on two brain regions

Quoting from the paper:

> > When interpreting our work we feel it is important to keep in mind what we see as a number of meaningful limitations. The central consideration in our view is the validity of the experimenter-imposed regulation conditions; while state induction is a fairly standard technique, it is very difficult to know whether our manipulation worked as intended. In particular it seems naïve to rule out the potentially large role of demand effects present in the laboratory context. We could expect that there is a heterogeneity of strategies employed by subjects that account for behavioral changes in the regulatory tasks, some of which are unaligned with the regulatory conditions we are attempting to examine. We are also aware that the time period over which the lasting effects of our manipulation are tested (ca. 60 minutes) is much too short to be meaningful in a real-world context. Testing the durability of the effect over longer timespans would help make clear whether our manipulations could be beneficial outside of the laboratory. 
> >
> >  
> >
> > Our results present one answer to why diets may not last: if regulation operates in part by temporarily recruiting the dlPFC to represent stimulus values in a goal-consistent way, but requires effort to maintain, then a loss of focus on the regulatory task may result in a rebound of previous preferences. Indeed, in our study, individuals who showed the strongest goal-consistent stimulus representations in the dlPFC during regulation were also the *least* likely to maintain the effects of regulation on post-regulation preferences. But this result also raises an important question for further research: if recruitment of the dlPFC predicts a failure of regulatory efforts to last beyond the moment of active focus, what mechanisms predict enduring change? In our study, we saw evidence that a focus on healthiness in the moment may result in increases in the value of healthy foods afterwards. We had predicted that the changes in the vmPFC might correlate with such effects, but they did not. One possibility is that more sophisticated, multivariate analyses might yield clearer correlates of enduring change. Another possibility is that lasting change might result from connectivity between value regions and regions associated with memory and reward learning (e.g., hippocampus and ventral striatum: Gerraty et al., 2014; Wimmer & Shohamy, 2012). Future work will be needed to test these ideas.
> >
> >  
> >
> > Finally, although we focused here on the vmPFC and dlPFC, based on a plethora of work implicating these two regions in the cognitive self-regulation of value, our research suggests the potential utility of considering a larger set of potential players. For example, other regions, including orbitofrontal cortex, ventral striatum, and posterior cingulate cortex, also correlate with decision values at the time of choice (Clithero & Rangel, 2013). Perhaps each of these regions also plays a unique role in regulatory success under the right circumstance. Given the multitude of possible strategies people might use to regulate their evaluative responses (Gross, 2015), future work should take a more global perspective on this important issue.



#### General

**R1.15**

> There are spelling and punctuation errors throughout the manuscript.

We have done a copyediting pass.



### Reviewer 2

#### Summary
> This interesting paper describes an fMRI study of food choice under three instruction conditions: a “NATURAL” instruction, a general “DECREASE” instruction, and a “HEALTH” instruction. While I appreciated the introduction, the multi-pronged investigative and analytic approach, and the careful hypotheses, several issues reduce my enthusiasm at this time.



#### Main Points

**R2.1**

> The most prominent issue for this reviewer was the use of post-task ratings of tastiness and healthiness as predictors of brain activity during the previously-administered task (pre-task ratings are not described and thus no pre- to post-task changes in healthiness and tastiness). Since the regulatory task conditions are known to influence such ratings (and in some cases like HEALTH, were designed to directly influence such ratings), the prediction would more appropriate in the opposite direction – whereby regulatory instructions led to neural changes (e.g., in dlPFC), which influenced post-task ratings. While some of the analyses/results may still be amenable to this order issue (e.g., decision value), others seem circular/reversed in their current form (e.g., delta liking, sensitivity, and descriptive/interpretive statements such as “tastiness influenced choices more strongly…” “lasting changes in concern for tastiness…”).     

The assumption implicit in our experimental design is that the attribute evaluations of our food options (e.g. health and taste)  are stable. Since we didn't collect pre-task ratings for the health and taste attributes we cannot guarantee that this is the case, and have added this to our enumeration of the experiment's limitations in our discussion. That said, we have conducted previous studies with similar designs, but where we did take healthiness and tastiness ratings twice. In these instances we found high stability/correlation within the attribute evaluations at the two timepoints [reference these papers?]. This assumption of stability is also consistent with other studies which also only take attribute ratings after the task manipulation, for example LIST OTHER STUDIES.

Our theoretical supposition is that it is the attribute *weights*, which represent the importance of the attribute in the decision process, that are being effected by our experimental manipulations, and not the attribute *evaluations*, which represent how good or bad the subject believes that attribute to be. If our belief is correct, then the liking rating represent the integration of the weighted attribute evaluations. That is to say that we believe that changes in our liking ratings capture changes in how subjects are using the attributes of taste and health to drive their choices, but in terms of changes in attribute weights, rather than evaluations. Our regression analysis, based on these changes in liking, then allows us to calculate these changes in attribute weight. That said, directly testing for this assumed stability in attribute evaluation would clearly strengthen our claim that changes in liking pre/post task are in fact solely driven by altered attribute weights.



**R2.2**

> One technical issue that made the results less easy to follow is that it was not always clear which values/equations was being used in each analysis. It would help if each equation in the methods was numbered, and then referred to explicitly in the results when it is used.

The equations have been numbered and REFERENCE IN RESULTS.



**R2.3**

> One conceptual issue is that the authors focus in both title and abstract on the idea of transient vs. lasting regulatory effects, which – once I got to the methods and results – I found to be potentially misleading. Indeed, the “lasting” aspect refers to changes in the “liking rating task” from pre- to post-fMRI, which still seems rather transient (even if not immediate, as changes occurring during the task itself). How long after the task was the second rating task? Perhaps the authors can clarify the timing and (unless this several is hours later) can tone down the “lasting” language (“immediate vs. delayed” may be a better fit?).

We think this is an important point and have clarified our language to refer to "momentary" and "persistent" effects of cognitive regulation, rather than transient and lasting. We have also added information about the duration of the persistent effects (M = 57.01 minutes, SD = 8.65 minutes).



**R2.4**

> This reviewer also noted some confusion between and interchangeable use of preference, liking, and wanting/craving. The authors describe a “liking rating task,” which participants completed both before and after the fMRI task. In this task, participants rated “how much they would like to eat the food right now.” In my humble opinion, this seems more consistent with a motivational “wanting” or craving question – especially in the presence of a food cue – even if the wording is “liking.” They also refer to these same ratings as preferences in multiple places including the abstract and discussion. This type of confusion is unfortunately a common issue in the literature (e.g., Pool et al 2016 in Neuroscience and Biobehavioral reviews, and see also Havermans 2012, Appetite), and would benefit from a brief acknowledgement, discussion, and clarification. It might be particularly relevant as the DECREASE instruction makes direct reference to decreasing craving, and likely affects those ratings (although this is not reported, see below, please report this in supplementary). 

DISCUSS WITH CENDRI



**R2.5**

> Psychologically, it is unclear to me what participants were doing during “DECREASE” trials. If there is a longer version of the instructions, it would be helpful to append it so the reader can evaluate. Further, did you collect any data on what the participants implemented during this condition? This might shed light on how participants were doing when they were told to “avoid any craving or emotional response to the food while choosing.” I’m not sure how I would implement this instruction myself.

We have added the directions subjects received in each condition to the supplementary materials. 

SHOULD WE MENTION THE DATA WE COLLECTED ON THE CONDITIONAL STRATEGIES THEY SELF-REPORTED?



**R2.6**

> Given the issues I noted so far, and until I have some clarifications, I am unable to review the discussion at this time, although it seems thoughtful and interesting.



#### Minor Points

**R2.7**

> The paper needs some copyediting. There are several issues with references (e.g., p. 6, p. 10), including some that do not appear in the reference section (e.g., Schmidt et al 2018, Rogers and Monsell, 1995). There are also some typos (e.g., “assocation” on p. 4).

We have done a copyediting pass and cleaned up/corrected the references. 

Started, need to complete

**R2.8**

> In the “stimuli” section, the authors describe using a set of 135 foods in two quantities, yielding 270 unique images. This set is described again almost in the same exact words in a subsequent section. This is unnecessary and confusing. Also, I have never considered clam juice to be a food, so this particular choice was slightly puzzling.

We have removed the redundant description of the food image set, and reminded readers in the Liking Rating Task section that a more complete description of the image set is provided in the Stimuli section.



**R2.9**

> What are the sources of the stimuli? As they are described to “cover the full range of healthiness and tastiness,” it would be good to clarify whether they were pre-piloted to cover this range and provide relevant data (even if in a supplementary material).

CHECKING WITH CENDRI



**R2.10**

> Some sentences in the results were confusing, especially where interactions are noted (e.g., p. 22 “interaction effect of condition”).

We have attempted to make this language more clear and straight forward.



**R2.11**

> It would be relevant to report the effect of condition on change in “liking” from baseline to post-task, especially given the instructions of the DECREASE condition.

TO DO



**R2.12**

> The correlational analysis on the bottom of p. 24 initially confused me. I thought it described a trial-by-trial analysis (“we asked whether goal-consistent changes in the vmPFC or dlPFC predict the durability or transience of regulatory effects”), but the stats suggest this was an individual difference analysis.

CHECK WITH CENDRI



**R2.13**

> The wording on the number of participants could use clarification – I believe you collected data from 64 participants and excluded 14 of those to end up with N=50? 

We attempted to make this clearer in the methods in our initial description of Participants.



**R2.14**

> It is interesting that you report a decrease in dlPFC during the DECREASE condition relative to NATURAL. While this is consistent with Hutcherson 2012, some others have reported increases in dlPFC during cognitive regulation in the presence of food (e.g., Kober 2010, Volkow 2011, Yokum 2013, Giuliani 2014, etc), and I would be curious to hear your thoughts on this.

NEED TO ADD COMMENT IN DISCUSSION



**R2.15**

> Additional analyses related to what people chose in each conditions, and questions related to amount would be nice to have in a supplementary results section.

THINKING ABOUT THIS



**R2.16**

> I personally found the use of “scanning sessions” in the methods to be confusing – I think you use “scanning session” to describe what I’ve typically seen described as “runs” within the same session. I have most often seen “scanning sessions” as describing fMRI scans taken on different days.

We have changed our language to refer to these as "runs".



**R2.17**

> On p. 10, “Foods were assigned to each condition for each subject such that baseline liking ratings were matched across conditions” – this is noted prior to describing the rating task, so when I first read it, I found it confusing. You might just want to add “(see below)”? This is not necessary, just a suggestion. 

Added.



**R2.18**

> On p. 11, “To examine how choice behavior changed during cognitive regulation” – you can’t technically measure change, but rather differences, especially as it refers to decision value.

Replaced "changed" with "differed".



**R2.19**

> Again in a supplement, it would be helpful to elaborate on what you did with frames showing displacement in runs that were not excluded.

WHAT DID WE DO? EXLUDE?



**R2.20**

> On p. 15, please clarify if in regressors R4-R6 the parametric modulators were for each specific trial or for the trial type as noted?

CENDRI THINKS EACH TRIAL BUT CHECK



**R2.21**

> On p. 15, how many motion parameters? Six? Six and their derivatives? Squares?

CENDRI TO CHECK



**R2.22**

> In the results, Figure 2 is never mentioned. Instead, I believe you use Figure 1 where you mean Figure 2 (e.g., p. 21, Figure 1a,c; Figure 1b).

This has been corrected. (WILL BE CORRECTED)

 

 

 

 